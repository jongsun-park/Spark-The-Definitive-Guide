{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/03 22:24:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|  red|good|    35|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로딩 & 샘플 확인\n",
    "df = spark.read.json(\"../data/simple-ml\")\n",
    "df.orderBy(\"value2\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 (예. 고객 건강 데이터셋)\n",
    "- color: 범주형 변수(red, blue, ...) -> 서비스 담당자가 작성한 고객의 건강 등급\n",
    "- lab: 범주형 레이블(good, bad) -> 실제 고객 건강 상태\n",
    "- value1: 수치형 변수 (웹 서비스내에서의 여러 활동을 숫자로 표현한 척도)\n",
    "- value2: 수치형 변수\n",
    "\n",
    "목표: 다른 값으로 이진변수(레이블)를 예측하는 분류 모델을 학습\n",
    "\n",
    "LIBSVM\n",
    "- *LIBSVM은 텍스트 기반의 희소 행렬(Sparse Matrix) 형식으로 데이터를 저장.*\n",
    "- *이 형식은 효율적인 데이터 저장과 빠른 연산을 위해 설계되었으며, 특히 SVM 모델 학습에 적합.*\n",
    "- 비어 있는 영역이 많고, 실제로 값이 있는 레이블이 제공된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.4.1 변환자를 사용해서 피처 엔지니어릴 수행하기\n",
    "\n",
    "변환자\n",
    "- 여러 방식으로 현재 칼럼을 조작하는데 사용\n",
    "- 모델의 입력변수로 사용할 특징을 개발하는 데 초점\n",
    "- 특징 수를 줄이거나, 특징을 추가하거나, 도출된 특징을 조작하거나, 데이터를 적절히 구성하는데 사용\n",
    "\n",
    "MLlib 머신러닝 알고리즘의 입력변수\n",
    "- Double 타입: 레이블용\n",
    "- Vector[Double] 타입: 특징용\n",
    "\n",
    "RFormula\n",
    "- 머신러닝에서 데이터 변환을 지정하기 위한 선언적 언어\n",
    "- 모델 및 데이터 조작을 위해 R 연산자의 한정된 부분 집합을 지원\n",
    "  - `~`: 함수에서 타깃과 항을 분리\n",
    "  - `+`: 연결 기호\n",
    "  - `-`: 삭제 기호\n",
    "  - `:`: 상호작용\n",
    "  - `.`: 타깃/종속변수를 제외한 모든 컬럼\n",
    "\n",
    "RFormula 클래스를 가져와서 수식을 정의\n",
    "- 예제. 모든 변수를 사용(.)하고, value1과 color, value2와 color 간의 상호작용을 추가하여 새로운 특징으로 처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 컬럼에 대한 가능한 값을 찾아내기 위해 RFormula 변환자를 데이터에 적합\n",
    "\n",
    "fit 메서드를 호출하면 실제로 데이터를 변형시키는 데 사용할 수 있는 학습된 버전의 변환자를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame을 준비\n",
    "fittedRF = supervised.fit(df)\n",
    "preparedDF = fittedRF.transform(df)\n",
    "preparedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features\n",
    "- 변환을 수행한 결과\n",
    "- 원시 데이터가 포함\n",
    "\n",
    "fit(): RFormula가 데이터를 검사하고 RformulaModel이라는 지정된 수식에 따라 데이터를 변환할 객체를 출력\n",
    "\n",
    "변환자\n",
    "- 범주형 변수를 Double 타입으로 변환\n",
    "- color -> 각 범주에 숫자값을 할당\n",
    "- color + value1, color + value2 -> 상호작용이 반영된 새로운 특징을 만듬\n",
    "- 모든 특징을 단일 벡터에 넣은 후 입력 데이터를 출력 데이터로 변환하기 위해 해당 객체에 대한 transform을 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 임의 분할 (학습셋, 테스트셋)\n",
    "train, test = preparedDF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.4.2 추정자\n",
    "\n",
    "예. 로지스틱 회귀 (분류 알고리즘)\n",
    "\n",
    "분류기 생성\n",
    "1. 로지스틱회귀 알고리즘 객체화 (기본 설정값)\n",
    "2. 레이블 칼럼과 특징 컬럼 설정\n",
    "  - 레이블 이름: label\n",
    "  - 특징 컬럼: features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 확인\n",
    "# 스파크를 이용하여 로지스틱 회귀를 구현했을 때 설정된 모든 파라미터의 설명이 출력\n",
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/03 23:11:39 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "# 알고리즘을 객체화한 다음 학습 데이터에 적합\n",
    "# 모델을 학습시키기 위한 작업을 수행하고, 학습된 모델을 반환\n",
    "fittedLR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform() 메서드를 사용하여 예측을 수행\n",
    "fittedLR.transform(train).select(\"label\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼파라미터\n",
    "- 모델 아키텍처 및 일반화(regularization)와 같은 학습 프로세스에 영향을 주는 설정 매개변수\n",
    "- 학습을 시작하기 전에 설정\n",
    "\n",
    "표준화(standarization)\n",
    "- 평균을 기준으로 관측값들이 얼마나 떨어져 있는지 재표현하는 방법\n",
    "- 특징의 단위가 2개 이상 다를 때 같은 기준으로 만들어 비교하거나 함께 다를 수 있도록 표준화를 사용\n",
    "- 예. 수치 데이터 표준화, 카테고리 데이터 표준화, 서수 데이터 표준화\n",
    "\n",
    "정규화(normalization)\n",
    "- 데이터의 범위를 바꾸는 방법\n",
    "  - 데이터 분포의 중심을 0으로 맞추고, \n",
    "  - 값의 분포가 특정 범위 안에 들어가도록 조정, \n",
    "  - 표준화, \n",
    "  - 모든 값을 0에서 1사이의 값으로 재표현\n",
    "\n",
    "일반화(regularization)\n",
    "- 모델 과적합을 방지하기 위한 기법\n",
    "- 모델의 표현식에 추가적인 제약 조건을 걸어 모델이 필요 이상으로 복잡해 지지 않도록 조정해주는 방법\n",
    "- 복잡한 커브를 많이 가지고 있는 가설함수를 가능한 한 부드럽고 단순하게 만들어주는 기법\n",
    "- 예. 리지 회귀, 라쏘, 엘라스틱넷, 최소각 회귀\n",
    "\n",
    "머신러닝\n",
    "- 표준화 및 정규화 - 스케일링 방법 - 데이터 전처리 과정\n",
    "- 일반화 - 모델의 일반화 오류를 줄여서 괒거합을 방지하기 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.4.3 워크플로를 파이프라인으로 만들기\n",
    "\n",
    "스파크 Pipeline\n",
    "- 마지막 단계인 추정자는 조건에 따라 자동으로 조정되며\n",
    "- 그에 따른 데이터 변환 과정을 설정할 수 있기 때문에\n",
    "- 튜닝된 모델을 바로 사용할 수 있다. \n",
    "\n",
    "![머신러닝 워크플로를 파이프라인으로 만들기](../images/24-4.png)\n",
    "\n",
    "다른 파이프라인을 생성하기 전에 항상 새로운 모델 객체를 만들어야 한다.\n",
    "\n",
    "홀드아웃 테스트셋 생성하고 검증셋을 기반으로 하이퍼파라미터를 조정해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습셋, 훈련셋 분리\n",
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이프라인의 기본 단계 생성 (변환자, 추정자)\n",
    "\n",
    "추정자\n",
    "1. RFomula: 입력 특징을 이해하기 위해 데이터를 분석하고 새로운 특징을 생성하기 위해 형상을 변형\n",
    "2. LogisticRegression: 모델 생성을 위해 학습을 하는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rForm = RFormula()\n",
    "lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 파이프라인에서 단계로 만듬\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stages = [rForm, lr]\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.4.4 모델 학습 평가\n",
    "\n",
    "논리적 파이프라인 -> 모델 학습 -> 평가기, 검증셋, 최적의 모델 선택\n",
    "- 여러 개의 모델을 학습 (다양한 하이퍼파리미터의 조합을 지정)\n",
    "\n",
    "RFormula\n",
    "- 전체 파이프라인에 다양한 하이퍼파라미터를 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "params = ParamGridBuilder()\\\n",
    "    .addGrid(rForm.formula, [\n",
    "        \"lab ~ . + color:value1\",\n",
    "        \"lab ~ . + color:value1 + color:value2\"])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .addGrid(lr.regParam, [0.1, 2.0])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제의 하이퍼파라미터\n",
    "1. 두 개의 서로 다른 버전의 RFormula\n",
    "2. 세 개의 서로 다른 옵션의 ElasticNet 파라미터\n",
    "3. 두 개의 서로 다른 옵션의 일반화 파라미터\n",
    "\n",
    "12개의 다른 파라미터 조합 -> 12가지 버전의 로지스틱 회귀를 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 프로세스 지정\n",
    "- 평가기: 자동으로 여러 모델을 동일한 평가지표를 적용하여 객관적으로 비교\n",
    "  - BinaryClassificationEvaluator\n",
    "  - 수신자 조작 특성(receiver operating charateristic) 아래의 areaUderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    "    .setMetricName(\"areaUnderROC\")\\\n",
    "    .setRawPredictionCol(\"prediction\")\\\n",
    "    .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이프라인: 데이터 변환 방법을 지정\n",
    "\n",
    "모델의 다양한 하이퍼파라미터를 시험하고 성능을 비교하여 모델 적합도를 측정하고 모델을 선택\n",
    "\n",
    "모델의 과적합을 방지하기 위해 테스트셋 대신 검증셋으로 하이퍼파라미터를 적합시키는 것이 좋다.\n",
    "- 홀드아웃 테스트셋을 사용할 수 없다.\n",
    "\n",
    "하이퍼파라미터 튜닝을 자동으로 수행하는 두 가지 옵션 제공\n",
    "- `TrainValiadationSplit`: 데이터를 두 개의 서로 다른 그룹으로 무작위 임의 분할할 때 사용\n",
    "- `CrossValidator`: 데이터 집합을 겹치지 않게 임의로 구분된 k개의 폴드로 분할하여 K-겹 교차검증을 수행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "tvs = TrainValidationSplit()\\\n",
    "    .setTrainRatio(0.75)\\\n",
    "    .setEstimatorParamMaps(params)\\\n",
    "    .setEstimator(pipeline)\\\n",
    "    .setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이프라인 구동\n",
    "- 검증셋에 대해 모든 버전의 모든 모델이 테스트된다.\n",
    "- 주어진 모델을 적할시킬 때마다 모델 유형을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tvs = TrainValidationSplit\n",
    "tvsFitted = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 테스트셋에서 어떤 성능이 보이는지 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(tvsFitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일부 모델 학습 결과를 요약\n",
    "- 파이프라인에서 학습 결과를 추출하고\n",
    "- 적절한 유형으로 전달하여 결과를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6810971138759443, 0.48677034522777857, 0.4688225590478846, 0.4604254606220952, 0.4560909637769964, 0.454697425876232, 0.45469352805771984, 0.45469140181985324, 0.45469138765667355, 0.4546913849245738, 0.4546913846346825, 0.45469138461933123, 0.4546913846189616]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "# 최적의 파이프라인 모델 로드\n",
    "trained_pipeline = tvsFitted.bestModel  # PipelineModel\n",
    "\n",
    "# 파이프라인에서 로지스틱 회귀 모델 가져오기\n",
    "trained_lr = trained_pipeline.stages[1]  # LogisticRegressionModel\n",
    "\n",
    "# 모델 요약 정보 가져오기\n",
    "summary_lr = trained_lr.summary\n",
    "\n",
    "# 학습 과정에서의 손실 값 출력\n",
    "print(summary_lr.objectiveHistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.4.5 모델 저장 및 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 모델을 디스크에 저장\n",
    "tvsFitted.write().overwrite().save(\"../tmp/modelLocation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.4/libexec/python/lib/pyspark.zip/pyspark/daemon.py:154: DeprecationWarning: This process (pid=44352) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|color| lab|value1|            value2|            features|label|       rawPrediction|         probability|prediction|\n",
      "+-----+----+------+------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "| blue| bad|     8|14.386294994851129|(7,[2,3,6],[8.0,1...|  0.0|[2.0340876896385,...|[0.88432986730886...|       0.0|\n",
      "| blue| bad|     8|14.386294994851129|(7,[2,3,6],[8.0,1...|  0.0|[2.0340876896385,...|[0.88432986730886...|       0.0|\n",
      "| blue| bad|     8|14.386294994851129|(7,[2,3,6],[8.0,1...|  0.0|[2.0340876896385,...|[0.88432986730886...|       0.0|\n",
      "| blue| bad|    12|14.386294994851129|(7,[2,3,6],[12.0,...|  0.0|[2.35701370326278...|[0.91349010183322...|       0.0|\n",
      "| blue| bad|    12|14.386294994851129|(7,[2,3,6],[12.0,...|  0.0|[2.35701370326278...|[0.91349010183322...|       0.0|\n",
      "| blue| bad|    12|14.386294994851129|(7,[2,3,6],[12.0,...|  0.0|[2.35701370326278...|[0.91349010183322...|       0.0|\n",
      "|green| bad|    16|14.386294994851129|[0.0,1.0,16.0,14....|  0.0|[-0.1358747778149...|[0.46608346995241...|       1.0|\n",
      "|green|good|     1|14.386294994851129|[0.0,1.0,1.0,14.3...|  1.0|[-0.1833463564741...|[0.45429138383743...|       1.0|\n",
      "|green|good|     1|14.386294994851129|[0.0,1.0,1.0,14.3...|  1.0|[-0.1833463564741...|[0.45429138383743...|       1.0|\n",
      "|green|good|     1|14.386294994851129|[0.0,1.0,1.0,14.3...|  1.0|[-0.1833463564741...|[0.45429138383743...|       1.0|\n",
      "|green|good|     1|14.386294994851129|[0.0,1.0,1.0,14.3...|  1.0|[-0.1833463564741...|[0.45429138383743...|       1.0|\n",
      "|green|good|    12|14.386294994851129|[0.0,1.0,12.0,14....|  1.0|[-0.1485338654574...|[0.46293465418461...|       1.0|\n",
      "|green|good|    12|14.386294994851129|[0.0,1.0,12.0,14....|  1.0|[-0.1485338654574...|[0.46293465418461...|       1.0|\n",
      "|green|good|    12|14.386294994851129|[0.0,1.0,12.0,14....|  1.0|[-0.1485338654574...|[0.46293465418461...|       1.0|\n",
      "|green|good|    15| 38.97187133755819|[0.0,1.0,15.0,38....|  1.0|[-1.0568033942896...|[0.25792080395727...|       1.0|\n",
      "|green|good|    15| 38.97187133755819|[0.0,1.0,15.0,38....|  1.0|[-1.0568033942896...|[0.25792080395727...|       1.0|\n",
      "|green|good|    15| 38.97187133755819|[0.0,1.0,15.0,38....|  1.0|[-1.0568033942896...|[0.25792080395727...|       1.0|\n",
      "|green|good|    15| 38.97187133755819|[0.0,1.0,15.0,38....|  1.0|[-1.0568033942896...|[0.25792080395727...|       1.0|\n",
      "|  red| bad|     1| 38.97187133755819|[1.0,0.0,1.0,38.9...|  0.0|[1.13909891283533...|[0.75751415990398...|       0.0|\n",
      "|  red| bad|     1| 38.97187133755819|[1.0,0.0,1.0,38.9...|  0.0|[1.13909891283533...|[0.75751415990398...|       0.0|\n",
      "+-----+----+------+------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실제 예측을 수행하기 위해 다른 스파크 모델에 모델을 적재할 수 있다.\n",
    "# 특정 알고리즘에 대한 \"모델\" 버전을 사용하여 디스크에 저장된 모델을 불러와야 한다.\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplitModel\n",
    "\n",
    "# 저장된 모델 로드\n",
    "model = TrainValidationSplitModel.load(\"../tmp/modelLocation\")\n",
    "\n",
    "# 모델을 사용하여 예측 수행\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# 결과 출력\n",
    "predictions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
